<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>in</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="introduction">Introduction</h1>
<p>VisionSE is computer vision project from MPC Lab, Univesity of
California, Berkeley, aiming for implmenting a perception algorithm that
only relies on the RGB data from a windshield windshield camera to
estimate the motion (longitudinal velocity and angular velocity). Such a
task is quite meaningful, since it challenges the designer to optimize
the use of RGB information to an extreme level. We do not want this
algorithm to rely on any training (like all other learning methods) or
preknowledge about the envrionment (like SLAM).</p>
<p>In the first half of this project, we successfully implement an
algorithm based on optical flow for which the only information needed is
the height of the camera from the ground (h), the focal length (f), and
the instantaneous RGB data collected the camera. No GPS, depth camera,
or IMU is required. Also, the algorithm is completely unsupervised with
no need for training with data, or preconstruction of the environment.
The key idea of such pure-vision solution is a combination of optical
flow and leat-square regression based on physics model (see the
methodology part for details). As we know, the measurements of optical
flow is only reliable when there are enough good features to track
(sharp corners with ample intensity gradients). When we do the
experiment in an ideal situation (road with chessboard texture), this
pure-vision algorithm turns out to perform pretty well. However, in more
general road settings in which sharp-corner features appear randomly
with a low frequency, this algorithm turns out to be very unstable.</p>
<p>To overcome this flaw(unstability when we don’t have enough good
features to track), in the second half of this project, we choose to
fuse the pure-vision estimation with the imu estimations. When we are in
a scenario where optical flow is not applicable, imu is a great backup
solution in short-run. The estimation based on imu turns out to be a
great complement for the pure-vision estimation.</p>
<h1 id="problem-formulating">Problem Formulating</h1>
<p>Consider an ego-vehicle that moves on a flat road. Suppose that we
have enough lightness in the environment to ensure a decent vision. A
camera is attached to its front windshield, and the camera remains
relatively still to the car. Since the road is relatively flat, we can
assume that the optical axis is parallell to the ground. The camera
samples images with a frequency around 24HZ. All the information we have
is the image sequences, now we need to use the images to estimate the
vheicle’s <strong>V_long</strong>, and <strong>w</strong>.</p>
<h1 id="methodology">Methodology</h1>
<h3 id="1-optical-flow-and-lucas-kanade-method">1. Optical flow and
Lucas-Kanade Method</h3>
<p>Optical flow is the apparent changes that we observe in the
brightness pattern as the surrounding environment changes. In an ideal
situation, optical flow should be the same as the motion field, the
projection of the surrounding environment’s relative motion onto the
image plane.<br />
In this project, we choose to apply Lucas-Kanade Method to calculate
optical flow, which enforces the ideal condition that the optical flow
value remains constant within its small pixel neighborhood. Based on our
problem formulation above, such a condition is perfectly met.</p>
<p><img
src="https://github.com/user-attachments/assets/d2c99202-4b5d-4f54-be3e-ead5957d8502"
alt="result1" /></p>
<p>The green arrows in the image above represents the optical flow
values at its correponding pixels, which is very informative about the
relative motion between the surround envrionment and the
ego-vehicle.</p>
<h3 id="2-longuet-higgins-and-prazdnys-motion-field-model">2.
Longuet-Higgins and Prazdny’s motion field model</h3>
<p>Based on the assumption that the ground is flat, and the
Longuet-Higgins and Prazdny’s motion field model, we can now formulate
the relationship between real-world egomotion <span
class="math inline"><em>V</em><sub><em>l</em></sub><em>o</em><em>n</em><em>g</em></span>
<span class="math inline"><em>w</em></span> and the pixel system motion
<span class="math inline"><em>U</em><sub><em>x</em></sub></span> <span
class="math inline"><em>U</em><sub><em>y</em></sub></span>. For all the
points on the flat ground, their ego-motion and motion field satisfies
the following relationship:</p>
<p><span class="math inline">${U_x}_i = \frac{x_iy_i}{fh} * V_long + (f
+ \frac{x_i^2}{f}) * w (1)$</span></p>
<p><span class="math inline">${U_y}_i = \frac{y_i^2}{fh} * V_long + (f +
\frac{x_iy_i}{f}) * w (2)$</span></p>
<p>In the equation above, <span
class="math inline"><em>U</em><sub><em>x</em></sub></span> and <span
class="math inline"><em>U</em><sub><em>y</em></sub></span> are the x
component and y-component of the motion field value for a single pixel,
indexed as i. <span
class="math inline"><em>x</em><sub><em>i</em></sub></span> and <span
class="math inline"><em>y</em><sub><em>i</em></sub></span> denote the x
and y coordinate for the pixel on the image plane. <span
class="math inline"><em>f</em></span> is the focal length and <span
class="math inline"><em>h</em></span> is the height of the camera from
the ground.</p>
<h3 id="3-least-square-regression">3. Least Square Regression</h3>
<p>By applying the Lucas-Kanade Method and its corresponding ideal
assumption, we can now get the optical flow values for for a list of
selected pixels at a given frame. However, all the optical values
obtained here are quite noisy and may differ a lot from the real motion
field values, for reasons like:</p>
<ul>
<li>lack of diversity of pixel intensity gradients in the selected
pixel’s neighborhood</li>
<li>sudden change of color pattern in the environment</li>
<li>Low frequency of image sampling rate<br />
Denote the optical flow values obtained from Lucas-Kanade Method as
<span
class="math inline"><em>U</em><sub><em>x</em></sub><sub><em>i</em></sub>′</span>
and <span
class="math inline"><em>U</em><sub><em>y</em></sub><sub><em>i</em></sub>′</span>,
which differs from the real motion field values by some noisy terms
<span class="math inline"><em>δ</em><sub><em>x</em></sub></span> and
<span class="math inline"><em>δ</em><sub><em>y</em></sub></span>. Now we
can formulate two optimization problem in terms of least square: <img
src="https://github.com/user-attachments/assets/9514b303-98a2-4515-bc8d-51fcf6dc092e"
alt="Optimization_formula" /></li>
</ul>
<p>To better understand the optimization problem above, we can interpret
it as a simple learning problem, where <span
class="math inline"><em>U</em><sub><em>x</em></sub><sub><em>i</em></sub>′</span>
and <span
class="math inline"><em>U</em><sub><em>y</em></sub><sub><em>i</em></sub>′</span>
are the labels. Terms like, <span
class="math inline">$\frac{x_iy_i}{fh}$</span>, <span
class="math inline">$\frac{x_i^2}{f})$</span>, <span
class="math inline">$\frac{y_i^2}{fh}$</span>, and <span
class="math inline">$(f + \frac{x_iy_i}{f})$</span> are the data poitns
corresponding to the labels, and <span
class="math inline"><em>V</em><sub><em>l</em></sub><em>o</em><em>n</em><em>g</em></span>
and <span class="math inline"><em>w</em></span> are the parameters of
the function to be learned.<br />
After finding the optimized values for <span
class="math inline"><em>V</em><sub><em>l</em></sub><em>o</em><em>n</em><em>g</em><sub><em>x</em></sub></span>,
<span class="math inline"><em>w</em><sub><em>x</em></sub></span> (the
values from the optimization problem of x-diretction motion field) and
<span
class="math inline"><em>V</em><sub><em>l</em></sub><em>o</em><em>n</em><em>g</em><sub><em>y</em></sub></span>,
<span class="math inline"><em>w</em><sub><em>y</em></sub></span> (the
values from the optimiztation problem of y-direction motion field), we
average the results from the two optimization problems to find the final
answer.</p>
<h1 id="optimization-functions">Optimization functions</h1>
<h3 id="1-pre-filter">1. Pre-filter</h3>
<p>As mentioned above, we treat the ego-motion estimation process as a
regression problem, where each pixel works as a data point and the
motion values at the frame are the parameters that are going to be
figured out. Then, a natural question will be: how can we remove the
outliers from the data points to make such estimation as accurate as
possible?</p>
<p>Some pixel points in the image may provide catastrophic measurements
for flow values because of the lack of texture variety in its
surrounding neighborhood or a sudden change in the nearby light sources.
To remove these pixels and their corresponding flows from our data, we
apply the ego-motion information of the vehicles from near history (in
recent 0.1 seconds) to form an approximation for the current ego-motion,
<span
class="math inline"><em>V</em><em>l</em><sub><em>p</em><em>a</em><em>s</em><em>t</em></sub></span>
<span
class="math inline"><em>w</em><sub><em>p</em><em>a</em><em>s</em><em>t</em></sub></span>.
As motion state of the ego-vehicle will remain relatively stable in a
very short time period, such an estimation is accurate enough to form an
approximation to remove obvious outliers in the data points.</p>
<p>These motion values based on history is then plugged in equation (1)
and (2) to give an approximation for the flow values of each pixels at
the current frame, <span
class="math inline"><em>U</em><sub><em>x</em></sub><sub><em>i</em></sub>′, <em>U</em><sub><em>y</em></sub><sub><em>i</em></sub>′</span>
With such approxmation for the flow values, we can then judge whether
the measurement of flow value given by each pixel is good or not.<br />
<span class="math inline">$Q_i = \frac{|{U_x}_i' - {U_x}_i|}{|{U_x}_i'|}
+ \frac{|{U_y}_i' - {U_y}_i|}{|{U_y}_i'|} \forall i$</span><br />
The value <span
class="math inline"><em>Q</em><sub><em>i</em></sub></span> is a measure
of the pixel’s quality. It shows how close the flow value estimated by
the pixel in the current frame is to the approximation made by the
history information. If this value is higher than a user-defined
thershold, i.e. such a flow is way too far from the history
approximation, we will exclude such a pixel from regression. To avoid
the current measurement from overly converging to the hisotry, the
filtered estimation will not be used as history information for future
filtering. All the history information for filtering is ego-motion
measurements without any optimization method.</p>
<h3 id="2-past-fusion">2. Past-fusion</h3>
<p>Similar to the idea of pre-filter, we can treat the history
measurements of the vehicle’s ego-motion as a sensor independentfrom the
current frame. Then, we can apply simple complementary sensor fusion
function to the history information and the current measurement. Again,
the history information we use here is the measurements without any
optimization methods. <img
src="https://github.com/user-attachments/assets/31155f1d-4e37-42ad-b5c1-7b4e227de4da"
alt="optical_flow_flow_chart" /></p>
<h3 id="3-fusion-with-imu">3. Fusion with IMU</h3>
<p>As we can see in the experiment result above, when the car is driving
in a scenario with few good features to track, optical flow becomes very
usntable. We can interpret such unstability as a kind of low-frequency
noise with high magnitude(extreme driving scenario appears with a low
frequency, but as long as such scenario apepars, optical flow will be
totally out of work). In comparison, if we only rely IMU sensor to
estimate the ego-motion, we will see some high-frequency noise with
low-magnitude. Such opposite natures of noises in pure-vision
estimations and imu estimations make them perfect complementary sensors,
which can be fused together.<br />
<img
src="https://github.com/user-attachments/assets/a4cb30b9-9273-48d9-9f99-04afcf5f85e3"
alt="flow_chart" /></p>
<h1 id="experiment-result">Experiment Result</h1>
<h3 id="pure-vision-method-in-ideal-scenario">Pure-Vision Method in
ideal scenario</h3>
<p>Flow digram from the windshield camera<br />
<img
src="https://github.com/user-attachments/assets/13334ba9-fe96-4887-a93a-e6c1a560e6b2"
alt="pure_vision_ideal_flow" /></p>
<p>Experiment result<br />
<img width="755" alt="pure_vision_ideal_result" src="https://github.com/user-attachments/assets/8acfd583-2e4a-4c7b-bee1-864a5e001f6e" /></p>
<h3 id="pure-vision-method-in-general-scenario">Pure-Vision Method in
general scenario</h3>
<p>Flow digram from the windshield camera<br />
<img
src="https://github.com/user-attachments/assets/4489ecfc-066c-4713-83e5-74591e84ab39"
alt="real_flow" /></p>
<p>Experiment result with only the estimation from optical flow
presented<br />
<img
src="https://github.com/user-attachments/assets/33fca6cc-30d1-4c7f-9e28-c1a4f164efdf"
alt="pure_vision_real" /></p>
<p>Experiment result with an extra median filter applied<br />
<img
src="https://github.com/user-attachments/assets/21832bd0-3352-4600-8200-4f40f4ad815e"
alt="pure_vision_real_median" /></p>
<h3
id="fusion-of-pure-vision-method-and-imu-estimation-in-general-scenario">Fusion
of Pure-vision Method and IMU Estimation in general scenario</h3>
<p>Experiment result<br />
<img
src="https://github.com/user-attachments/assets/2044b889-4258-4ae1-9598-d4fdb674eb88"
alt="fusion_real_result" /></p>
<h1 id="advantages-of-this-algorithm">Advantages of this algorithm</h1>
<ul>
<li>No need to any external assistance such as GPS</li>
<li>No need for any pre-knowledge about the driving scenario</li>
<li>No need for training with data</li>
<li>Absolute explainability. Unlike network or other data-driven methods
that are more prevalent nowadays, the whole motion field model is
completely explanable in the traditional computer-vision knowledge
framework. Any defects or disadvantages of its performance can be
tracked back to the theoretical level.</li>
<li></li>
</ul>
<h3 id="contact">Contact</h3>
<p>All the descriptions above are a brief summation of the key ideas of
my work. There are many details in the codes and tedious optimization
functions used in this project. If you are Interested in the details of
this project, feel free to email:<br />
liu.yx@berkeley.edu</p>
</body>
</html>
